"""
Regtag Pronunciation Engine - Phi√™n √¢m ch√≠nh x√°c t·ª´ regtag_v3.json

Engine phi√™n √¢m s·∫°ch v√† hi·ªáu qu·∫£ v·ªõi ∆∞u ti√™n tuy·ªát ƒë·ªëi cho regtag_v3.json.
Lu√¥n l·∫•y phi√™n √¢m ƒë·∫ßu ti√™n (chu·∫©n nh·∫•t) cho m·ªói t·ª´.

∆Øu ti√™n x·ª≠ l√Ω:
1. Cache (t·ªëc ƒë·ªô)
2. regtag_v3.json (ƒë·ªô ch√≠nh x√°c cao nh·∫•t - 128k+ t·ª´)
3. pronunciation_rules.json (backup cho t·ª´ chuy√™n ng√†nh)
4. Fallback (gi·ªØ nguy√™n t·ª´ g·ªëc)

Author: Dataset Generator Team
"""

import json
import os
from typing import Dict, List, Optional
import logging


class RegtagPronunciationEngine:
    """
    Engine phi√™n √¢m ch√≠nh x√°c v·ªõi logic r√µ r√†ng
    
    ∆Øu ti√™n x·ª≠ l√Ω:
    1. regtag_v3.json (∆Øu ti√™n tuy·ªát ƒë·ªëi - l·∫•y phi√™n √¢m ƒë·∫ßu ti√™n)
    2. pronunciation_rules.json (backup cho t·ª´ chuy√™n ng√†nh)
    3. Cache (t·ªëc ƒë·ªô)
    4. Fallback (gi·∫£i ph√°p cu·ªëi c√πng)
    
    T√≠nh nƒÉng:
    - L·∫•y phi√™n √¢m ch√≠nh x√°c t·ª´ 128k+ t·ª´ trong regtag_v3.json
    - Th·ªëng k√™ chi ti·∫øt ngu·ªìn phi√™n √¢m
    - B√°o c√°o t·ª´ thi·∫øu trong dictionary
    """
    
    def __init__(self, 
                 regtag_path: str = '/home/hiennt/Downloads/regtag_v3.json',
                 rules_file: str = './data/pronunciation_rules.json',
                 cache_file: str = './data/pronunciation_cache.json'):
        """Initialize with regtag as absolute priority"""
        self.regtag_path = regtag_path
        self.rules_file = rules_file
        self.cache_file = cache_file
        
        # Load data
        self.regtag = self._load_regtag()
        self.rules = self._load_rules()
        self.cache = {}  # In-memory cache only
        
        # Statistics
        self.stats = {
            'total_calls': 0,
            'regtag_hits': 0,
            'rules_hits': 0,
            'cache_hits': 0,
            'fallback_used': 0
        }
        
        # Logging
        logging.basicConfig(level=logging.WARNING)
        self.logger = logging.getLogger(__name__)
        
        print(f"‚úÖ RegtagPronunciationEngine initialized:")
        print(f"   üìö Regtag: {len(self.regtag)} words")
        print(f"   üìñ Rules:  {len(self.rules.get('common_words', {}))} words")
    
    def _load_regtag(self) -> Dict[str, str]:
        """
        Load regtag_v3.json
        LU√îN L·∫§Y PHI√äN √ÇM ƒê·∫¶U TI√äN (index 0)
        """
        if not os.path.exists(self.regtag_path):
            self.logger.warning(f"regtag_v3.json not found at {self.regtag_path}")
            return {}
        
        try:
            with open(self.regtag_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Process: word -> first pronunciation
            processed = {}
            for word, pronunciations in data.items():
                if pronunciations and isinstance(pronunciations, list) and len(pronunciations) > 0:
                    # LU√îN L·∫§Y PHI√äN √ÇM ƒê·∫¶U TI√äN
                    processed[word.lower()] = pronunciations[0]
            
            return processed
            
        except Exception as e:
            self.logger.error(f"Error loading regtag_v3.json: {e}")
            return {}
    
    def _load_rules(self) -> Dict:
        """Load pronunciation_rules.json as backup only"""
        if not os.path.exists(self.rules_file):
            return {"common_words": {}}
        
        try:
            with open(self.rules_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {"common_words": {}}
    
    def transcribe(self, word: str) -> str:
        """
        Transcribe English word to Vietnamese pronunciation
        
        Priority (STRICT):
        1. Cache (nhanh nh·∫•t)
        2. Regtag_v3.json (ABSOLUTE PRIORITY)
        3. Pronunciation_rules.json (backup)
        4. Fallback (cu·ªëi c√πng)
        
        Returns:
            Vietnamese pronunciation (string only)
        """
        self.stats['total_calls'] += 1
        
        # 1. Check in-memory cache
        if word in self.cache:
            self.stats['cache_hits'] += 1
            return self.cache[word]
        
        word_lower = word.lower()
        result = None
        source = None
        
        # 2. Check REGTAG FIRST (ABSOLUTE PRIORITY)
        if word_lower in self.regtag:
            result = self.regtag[word_lower]
            source = 'regtag'
            self.stats['regtag_hits'] += 1
        
        # 3. Check pronunciation rules (ONLY if not in regtag)
        elif word_lower in self.rules.get('common_words', {}):
            result = self.rules['common_words'][word_lower]
            source = 'rules'
            self.stats['rules_hits'] += 1
        
        # 4. Fallback (last resort)
        else:
            result = self._simple_fallback(word_lower)
            source = 'fallback'
            self.stats['fallback_used'] += 1
            self.logger.warning(f"Using fallback for: '{word}' ‚Üí '{result}'")
        
        # Cache result
        self.cache[word] = result
        
        # Debug log
        self.logger.debug(f"[{source}] {word} ‚Üí {result}")
        
        return result
    
    def _simple_fallback(self, word: str) -> str:
        """
        Simple fallback for unknown words
        Just return lowercase - let human review later
        """
        # C∆° b·∫£n: gi·ªØ nguy√™n lowercase
        # Trong production, t·ª´ n√†y s·∫Ω ƒë∆∞·ª£c human review
        return word.lower()
    
    def transcribe_multiple(self, words: List[str]) -> List[str]:
        """Transcribe multiple words"""
        return [self.transcribe(word) for word in words]
    
    def get_stats(self) -> Dict:
        """Get detailed statistics"""
        total = max(self.stats['total_calls'], 1)  # Avoid division by zero
        
        return {
            **self.stats,
            'regtag_percentage': (self.stats['regtag_hits'] / total) * 100,
            'rules_percentage': (self.stats['rules_hits'] / total) * 100,
            'cache_percentage': (self.stats['cache_hits'] / total) * 100,
            'fallback_percentage': (self.stats['fallback_used'] / total) * 100
        }
    
    def report_missing_words(self) -> List[str]:
        """
        Report words that used fallback
        These need to be added to dictionaries
        """
        missing = []
        for word, pron in self.cache.items():
            word_lower = word.lower()
            # If pronunciation equals word (fallback case)
            if pron == word_lower:
                missing.append(word)
        return missing
    
    def get_pronunciation_source(self, word: str) -> str:
        """Debug: check where pronunciation comes from"""
        word_lower = word.lower()
        if word_lower in self.regtag:
            return f"regtag: {self.regtag[word_lower]}"
        elif word_lower in self.rules.get('common_words', {}):
            return f"rules: {self.rules['common_words'][word_lower]}"
        else:
            return "fallback (not found)"


# Test
if __name__ == "__main__":
    print("="*60)
    print("TESTING RegtagPronunciationEngine")
    print("="*60)
    
    engine = RegtagPronunciationEngine()
    
    # Test c√°c t·ª´ c√≥ v·∫•n ƒë·ªÅ tr∆∞·ªõc ƒë√¢y
    test_words = [
        'task', 'product', 'user', 'experience',
        'pipeline', 'sync', 'align', 'optimize', 'system',
        'campaign', 'workload', 'KPI', 'Q3'
    ]
    
    print("\n" + "="*60)
    print("TRANSCRIPTION RESULTS")
    print("="*60)
    
    for word in test_words:
        result = engine.transcribe(word)
        source = engine.get_pronunciation_source(word)
        print(f"{word:15} ‚Üí {result:25} [{source}]")
    
    print("\n" + "="*60)
    print("STATISTICS")
    print("="*60)
    
    stats = engine.get_stats()
    print(f"Total words:     {stats['total_calls']}")
    print(f"From regtag:     {stats['regtag_hits']} ({stats['regtag_percentage']:.1f}%)")
    print(f"From rules:      {stats['rules_hits']} ({stats['rules_percentage']:.1f}%)")
    print(f"From cache:      {stats['cache_hits']} ({stats['cache_percentage']:.1f}%)")
    print(f"Fallback:        {stats['fallback_used']} ({stats['fallback_percentage']:.1f}%)")
    
    missing = engine.report_missing_words()
    if missing:
        print(f"\n‚ö†Ô∏è  Missing words ({len(missing)}): {missing}")
    
    print("\n" + "="*60)
